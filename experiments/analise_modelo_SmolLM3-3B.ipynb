{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d31edec",
   "metadata": {},
   "source": [
    "# Objetivo Experimental\n",
    "\n",
    "Para 500 amostras:\n",
    "\n",
    "- Accuracy\n",
    "- Latência média\n",
    "- Tokens processados\n",
    "- Ganho por custo computacional\n",
    "\n",
    "O ganho por custo computavional é definida por:\n",
    "\n",
    "$$\n",
    "Efficiency\\ Gain = \\frac{\\Delta Accuracy}{\\Delta Tokens}\n",
    "$$\n",
    "\n",
    "Ou seja:\n",
    "\n",
    "* Quanto de ganho em acurácia você obteve\n",
    "* Por token adicional processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a2139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641462f6",
   "metadata": {},
   "source": [
    "## 1. Carregando Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c82eb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6513e111fe41d4b846119528e2b0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SmolLM3ForCausalLM(\n",
       "  (model): SmolLM3Model(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x SmolLM3DecoderLayer(\n",
       "        (self_attn): SmolLM3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SmolLM3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): SmolLM3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): SmolLM3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): SmolLM3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): SmolLM3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    dtype=torch.float16, \n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c67a51",
   "metadata": {},
   "source": [
    "## 2. Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fac5298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'texto', 'sentimento'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"celsowm/imdb-reviews-pt-br\")\n",
    "test_data = dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe935d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 253, 0: 247})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(test_data[\"sentimento\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e75a0d",
   "metadata": {},
   "source": [
    "## 3. Função de classificação + métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338479ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_measure(text, repeated=False):\n",
    "    \n",
    "    base = f\"\"\"Classifique o sentimento como positivo ou negativo.\n",
    "Texto: {text}\"\"\"\n",
    "    \n",
    "    if repeated:\n",
    "        prompt = base + \"\\n\\n\" + base + \"\\nSentimento:\"\n",
    "    else:\n",
    "        prompt = base + \"\\nSentimento:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    latency = time.time() - start\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prediction = output_text.split(\"Sentimento:\")[-1].strip().lower()\n",
    "    \n",
    "    input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    return prediction, latency, input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09a951",
   "metadata": {},
   "source": [
    "## 4. Loop Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6c43cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 500/500 [05:36<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_normal = 0\n",
    "correct_repeated = 0\n",
    "\n",
    "latencies_normal = []\n",
    "latencies_repeated = []\n",
    "\n",
    "tokens_normal = []\n",
    "tokens_repeated = []\n",
    "\n",
    "results = []\n",
    "\n",
    "for example in tqdm(test_data):\n",
    "    \n",
    "    text = example[\"texto\"]\n",
    "    label = \"positivo\" if example[\"sentimento\"] == 1 else \"negativo\"\n",
    "    \n",
    "    # Normal\n",
    "    pred_n, lat_n, tok_n = classify_and_measure(text, repeated=False)\n",
    "    \n",
    "    # Repeated\n",
    "    pred_r, lat_r, tok_r = classify_and_measure(text, repeated=True)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results.append([label, pred_n, pred_r])\n",
    "\n",
    "    if label in pred_n:\n",
    "        correct_normal += 1\n",
    "        \n",
    "    if label in pred_r:\n",
    "        correct_repeated += 1\n",
    "    \n",
    "    latencies_normal.append(lat_n)\n",
    "    latencies_repeated.append(lat_r)\n",
    "    \n",
    "    tokens_normal.append(tok_n)\n",
    "    tokens_repeated.append(tok_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d310f1",
   "metadata": {},
   "source": [
    "## 5. Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d46902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== RESULTADOS =====\n",
      "Accuracy Normal: 0.8\n",
      "Accuracy Repeated: 0.882\n",
      "Δ Accuracy: 0.08199999999999996\n",
      "\n",
      "Latência Média Normal: 0.262154661655426\n",
      "Latência Média Repeated: 0.40699334812164306\n",
      "\n",
      "Tokens Médios Normal: 366.404\n",
      "Tokens Médios Repeated: 729.804\n",
      "\n",
      "Ganho por Token: 0.00022564667033571811\n"
     ]
    }
   ],
   "source": [
    "acc_normal = correct_normal / 500\n",
    "acc_repeated = correct_repeated / 500\n",
    "\n",
    "avg_latency_normal = np.mean(latencies_normal)\n",
    "avg_latency_repeated = np.mean(latencies_repeated)\n",
    "\n",
    "avg_tokens_normal = np.mean(tokens_normal)\n",
    "avg_tokens_repeated = np.mean(tokens_repeated)\n",
    "\n",
    "delta_acc = acc_repeated - acc_normal\n",
    "delta_tokens = avg_tokens_repeated - avg_tokens_normal\n",
    "\n",
    "efficiency_gain = delta_acc / delta_tokens\n",
    "\n",
    "print(\"===== RESULTADOS =====\")\n",
    "print(\"Accuracy Normal:\", acc_normal)\n",
    "print(\"Accuracy Repeated:\", acc_repeated)\n",
    "print(\"Δ Accuracy:\", delta_acc)\n",
    "\n",
    "print(\"\\nLatência Média Normal:\", avg_latency_normal)\n",
    "print(\"Latência Média Repeated:\", avg_latency_repeated)\n",
    "\n",
    "print(\"\\nTokens Médios Normal:\", avg_tokens_normal)\n",
    "print(\"Tokens Médios Repeated:\", avg_tokens_repeated)\n",
    "\n",
    "print(\"\\nGanho por Token:\", efficiency_gain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4314841",
   "metadata": {},
   "source": [
    "# Teste de McNemar\n",
    "\n",
    "Ele testa se a diferença entre dois classificadores avaliados nas mesmas amostras é estatisticamente significativa.\n",
    "\n",
    "## O que precisamos\n",
    "\n",
    "Para cada uma das amostras, precisamos saber:\n",
    "\n",
    "| Caso | Normal | Repeated |\n",
    "| ---- | ------ | -------- |\n",
    "| A    | ✔️     | ✔️       |\n",
    "| B    | ✔️     | ❌        |\n",
    "| C    | ❌      | ✔️       |\n",
    "| D    | ❌      | ❌        |\n",
    "\n",
    "\n",
    "O teste usa apenas:\n",
    "\n",
    "* b = Normal correto, Repeated errado\n",
    "* c = Normal errado, Repeated correto\n",
    "\n",
    "Estatística:\n",
    "\n",
    "$$\n",
    "X^2 = \\frac{(|b - c| -1)^2}{b + c}\n",
    "$$\t​\n",
    "\n",
    "\n",
    "## Interpretação\n",
    "\n",
    "Se:\n",
    "\n",
    "* p < 0.05 → diferença significativa\n",
    "* p < 0.01 → fortemente significativa\n",
    "* p < 0.001 → muito forte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd02f31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: 47\n",
      "c: 88\n",
      "statistic: 11.851851851851851\n",
      "p-value: 0.0005760403386062825\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# construir tabela de contingência\n",
    "b = 0  # normal certo, repeated errado\n",
    "c = 0  # normal errado, repeated certo\n",
    "\n",
    "for example in results:  \n",
    "    label, pred_n, pred_r = example\n",
    "    \n",
    "    normal_correct = label in pred_n\n",
    "    repeated_correct = label in pred_r\n",
    "    \n",
    "    if normal_correct and not repeated_correct:\n",
    "        b += 1\n",
    "    elif not normal_correct and repeated_correct:\n",
    "        c += 1\n",
    "\n",
    "table = [[0, b],\n",
    "         [c, 0]]\n",
    "\n",
    "result = mcnemar(table, exact=False, correction=True)\n",
    "\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"statistic:\", result.statistic)\n",
    "print(\"p-value:\", result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808b5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-repetition-effect (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
